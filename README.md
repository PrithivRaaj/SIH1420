# SIH1420
Problem Statement Title:

Lean module for reasoning about computational complexity in GPTs.

Description:

Formalization of mathematics and computer science is in vogue. Formalization means to express mathematical concepts, definitions, theorems, and proofs in a way that can be checked by a computer for correctness. Lean theorem prover is interactive and based on dependent type theory which is a powerful and expressive framework for formalizing computer science. It has been used for example in formalization of number theory. Similarly, it can be used to formalize the notion of computational complexity of generalized probabilistic theories (GPTs). Some of the relevant computational complexity classes are BGP (=AWPP), BQP, BPP, PP, PSPACE. We can formalize the notions of these computational complexity classes and the relations among them in lean. We can also move further on to formalize the notion of higher order interference in physical theories and formalize some theorems related to it. The exact problem is to write relevant lean module(s) containing the basic definitions, results in GPTs and tools for reasoning about computational complexity in GPTs. The developed module can further be used to give formal proofs for theorems and lemmas.

Abstract :

As the field of artificial intelligence progresses, understanding and managing the computational complexity of large language models, such as Generative Pre-trained Transformers (GPTs), becomes increasingly crucial. These models have demonstrated remarkable capabilities in various natural language processing tasks but often come with significant computational demands. Efficiently reasoning about and managing their computational complexity is essential for practical deployment and further advancements in AI research.This project introduces a lean module designed specifically for reasoning about computational complexity in GPTs. The module provides a systematic framework for assessing and predicting the computational resources required for different tasks and model configurations. Leveraging principles from algorithmic analysis and complexity theory, the module offers insights into the factors influencing computational complexity in GPTs, including model size, input length, and task complexity. This module offers a systematic framework for assessing and predicting the computational resources needed for various tasks and model configurations. By drawing on principles from algorithmic analysis and complexity theory, the module provides insights into the factors influencing computational complexity in GPTs, such as model size, input length, and task complexity. Key features of our proposed module include structured algorithmic analysis, detailed examination of model configurations, task-specific complexity assessment, and practical guidelines for managing computational complexity in GPT-based applications. Our aim is to empower AI researchers, developers, and practitioners with a practical toolset for effectively addressing the computational challenges associated with GPTs, thereby facilitating their responsible deployment and further advancements in realworld applications


The below picture is the outcome of the project 


![image](https://github.com/PrithivRaaj/SIH1420/assets/111727780/309393bd-dc59-4ed5-afde-73a9b819aeab)
